\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\abx@aux@refcontext{anyt/global//global/global}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\@newglossary{acronym}{alg}{acr}{acn}
\providecommand \oddpage@label [2]{}
\babel@aux{british}{}
\@writefile{toc}{\contentsline {chapter}{Abstract}{v}{Doc-Start}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Theoretical background}{2}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Neural Networks}{2}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Structure of Neural Networks}{2}{subsection.2.1.1}\protected@file@percent }
\abx@aux@cite{0}{ReLuGrad}
\abx@aux@segm{0}{0}{ReLuGrad}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Simple neural network consisting of a single hidden layer.\relax }}{3}{figure.caption.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Learning in Neural Networks}{3}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Activation Functions}{3}{subsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Supervised and unsupervised learning}{3}{subsection.2.1.4}\protected@file@percent }
\abx@aux@cite{0}{CNNs}
\abx@aux@segm{0}{0}{CNNs}
\abx@aux@cite{0}{RiebesellTikZ2022}
\abx@aux@segm{0}{0}{RiebesellTikZ2022}
\abx@aux@cite{0}{RiebesellTikZ2022}
\abx@aux@segm{0}{0}{RiebesellTikZ2022}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Convolutional Neural Networks, CNN}{4}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Convolutional layers}{4}{subsection.2.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Example of convolutional operation. $\mathbf  {I}$ is the input, $\mathbf  {K}$ is the kernel and $\mathbf  {I} * \mathbf  {K}$ is the activation map. Illustration taken from the Random TikZ collection\blx@tocontentsinit {0}\cite {RiebesellTikZ2022}\relax }}{4}{figure.caption.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Pooling}{5}{subsection.2.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Illustration of a max pooling operation. The input feature map is reduced in size by applying a max pooling filter with size 2x2 (red boxes), which selects the maximum value in each filter region to produce the max pooled feature map.\relax }}{5}{figure.caption.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Architecture of CNNs}{5}{subsection.2.2.3}\protected@file@percent }
\abx@aux@cite{0}{batchnorm}
\abx@aux@segm{0}{0}{batchnorm}
\abx@aux@cite{0}{ResLearn}
\abx@aux@segm{0}{0}{ResLearn}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Encoder}{6}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Downsampling blocks}{6}{subsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Residual block}{6}{subsection.2.3.2}\protected@file@percent }
\abx@aux@cite{0}{ResLearn}
\abx@aux@segm{0}{0}{ResLearn}
\abx@aux@cite{0}{ResLearn}
\abx@aux@segm{0}{0}{ResLearn}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Illustration of the Resnet block from He et al's original paper\blx@tocontentsinit {0}\cite {ResLearn} \relax }}{7}{figure.caption.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Encoder Architecture}{7}{subsection.2.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Illustration of the Encoder architecture used in the VQVAE implementation. The first downsample block reduces the dimensionality, followed by further downsample blocks to capture important features.\relax }}{8}{figure.caption.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Decoder}{8}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Upsampling Blocks}{8}{subsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Decoder Architecture}{8}{subsection.2.4.2}\protected@file@percent }
\abx@aux@cite{0}{MoCo}
\abx@aux@segm{0}{0}{MoCo}
\abx@aux@cite{0}{SimCLR}
\abx@aux@segm{0}{0}{SimCLR}
\abx@aux@cite{0}{BYOL}
\abx@aux@segm{0}{0}{BYOL}
\abx@aux@cite{0}{Barlow}
\abx@aux@segm{0}{0}{Barlow}
\abx@aux@cite{0}{Siamese}
\abx@aux@segm{0}{0}{Siamese}
\abx@aux@cite{0}{SSLs}
\abx@aux@segm{0}{0}{SSLs}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Self Supervised Learning, SSL}{9}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Contrastive Learning in SSL}{9}{subsection.2.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Non-Contrastive Learning in SSL}{9}{subsection.2.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}The Role of Siamese Networks in SSL}{9}{subsection.2.5.3}\protected@file@percent }
\abx@aux@cite{0}{svm}
\abx@aux@segm{0}{0}{svm}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces  Example of a non contrastive siamese network using augmented views. Here the images are processed to calculate a similarity score, the backpropagation arrow indicates that the similarity score is then used to update the weights of the sub network. Pictures taken from \url  {www.pexels.com}\relax }}{10}{figure.caption.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.4}Projector}{10}{subsection.2.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Projector architecture}{10}{section*.7}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{Bit vague}{10}{section*.8}\protected@file@percent }
\pgfsyspdfmark {pgfid3}{9791217}{21276430}
\pgfsyspdfmark {pgfid6}{36067557}{21278942}
\pgfsyspdfmark {pgfid7}{38009087}{21045537}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Additional Machine Learning Algorithms}{11}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Support Vector Machines, SVM}{11}{subsection.2.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Vizualisation of the hyperplane, margin and support vectors in a SVM procedure.\relax }}{11}{figure.caption.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}K-Neares Neighbors, KNN}{11}{subsection.2.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Classification of middle point in a KNN procedure. For $k=2$ the majority vote is purple while for $k=5$ it is blue.\relax }}{12}{figure.caption.10}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:knn}{{2.8}{12}{Classification of middle point in a KNN procedure. For $k=2$ the majority vote is purple while for $k=5$ it is blue.\relax }{figure.caption.10}{}}
\newlabel{fig:knn@cref}{{[figure][8][2]2.8}{[1][11][]12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}KMeans and Silhouette Score}{12}{subsection.2.6.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Illustration of the silhuette score metric applied. On the left we see well seperated clusters with a silhouette score equal to 1. On the right a not so well seperation, giving a silhouette score less than 1.\relax }}{13}{figure.caption.11}\protected@file@percent }
\newlabel{fig:knn}{{2.9}{13}{Illustration of the silhuette score metric applied. On the left we see well seperated clusters with a silhouette score equal to 1. On the right a not so well seperation, giving a silhouette score less than 1.\relax }{figure.caption.11}{}}
\newlabel{fig:knn@cref}{{[figure][9][2]2.9}{[1][12][]13}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Representation Learning}{13}{section.2.7}\protected@file@percent }
\abx@aux@cite{0}{neuvqvae}
\abx@aux@segm{0}{0}{neuvqvae}
\abx@aux@cite{0}{VQVAE-2}
\abx@aux@segm{0}{0}{VQVAE-2}
\abx@aux@cite{0}{lee2023masked}
\abx@aux@segm{0}{0}{lee2023masked}
\abx@aux@cite{0}{cvae}
\abx@aux@segm{0}{0}{cvae}
\abx@aux@cite{0}{Barlow}
\abx@aux@segm{0}{0}{Barlow}
\abx@aux@cite{0}{SSLs}
\abx@aux@segm{0}{0}{SSLs}
\abx@aux@cite{0}{UCRArchive2018}
\abx@aux@segm{0}{0}{UCRArchive2018}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Related work / Literature review}{14}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{tdo}{\contentsline {todo}{fill}{14}{section*.12}\protected@file@percent }
\pgfsyspdfmark {pgfid8}{10734166}{21364465}
\pgfsyspdfmark {pgfid11}{36067557}{21366977}
\pgfsyspdfmark {pgfid12}{38009087}{21133572}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Methology}{16}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Overview}{16}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}The Vector Quantized Variational Auto-Encoder, VQVAE}{16}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Evolution from Variational Auto-Encoder, VAE}{16}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Variational inference and optimization}{17}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Challenges in Maximum Likelihood Estimation}{17}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Variational Approximation}{17}{section*.15}\protected@file@percent }
\abx@aux@cite{0}{VAE}
\abx@aux@segm{0}{0}{VAE}
\abx@aux@cite{0}{1312.6114}
\abx@aux@segm{0}{0}{1312.6114}
\@writefile{toc}{\contentsline {subsubsection}{Evidence Lower Bound}{18}{section*.16}\protected@file@percent }
\newlabel{eq:ELBO}{{4.5}{18}{Evidence Lower Bound}{equation.4.5}{}}
\newlabel{eq:ELBO@cref}{{[equation][5][4]4.5}{[1][18][]18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Transition to discrete latent variables}{18}{subsection.4.2.2}\protected@file@percent }
\abx@aux@cite{0}{neuvqvae}
\abx@aux@segm{0}{0}{neuvqvae}
\abx@aux@cite{0}{neuvqvae}
\abx@aux@segm{0}{0}{neuvqvae}
\@writefile{toc}{\contentsline {subsubsection}{Vector quantization}{19}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Effect on KL divergence}{19}{section*.18}\protected@file@percent }
\abx@aux@cite{0}{posteriorcollapse}
\abx@aux@segm{0}{0}{posteriorcollapse}
\abx@aux@cite{0}{lee2023masked}
\abx@aux@segm{0}{0}{lee2023masked}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}VQVAE implementation}{20}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Informational flow}{20}{subsection.4.3.1}\protected@file@percent }
\abx@aux@cite{0}{lee2023masked}
\abx@aux@segm{0}{0}{lee2023masked}
\abx@aux@cite{0}{lee2023masked}
\abx@aux@segm{0}{0}{lee2023masked}
\abx@aux@cite{0}{lee2023masked}
\abx@aux@segm{0}{0}{lee2023masked}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces  Illustration of the VQVAE. Illustration and implementation inspired by the TimeVQVAE paper\blx@tocontentsinit {0}\cite {lee2023masked} \relax }}{21}{figure.caption.19}\protected@file@percent }
\newlabel{fig:VQVAE}{{4.1}{21}{Illustration of the VQVAE. Illustration and implementation inspired by the TimeVQVAE paper\cite {lee2023masked} \relax }{figure.caption.19}{}}
\newlabel{fig:VQVAE@cref}{{[figure][1][4]4.1}{[1][21][]21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Learning}{21}{subsection.4.3.2}\protected@file@percent }
\newlabel{eq:VQVAEloss}{{4.9}{21}{Learning}{equation.4.9}{}}
\newlabel{eq:VQVAEloss@cref}{{[equation][9][4]4.9}{[1][21][]21}}
\@writefile{toc}{\contentsline {subsubsection}{Reconstruction}{22}{section*.20}\protected@file@percent }
\newlabel{eq:recon}{{4.12}{22}{Reconstruction}{equation.4.12}{}}
\newlabel{eq:recon@cref}{{[equation][12][4]4.12}{[1][22][]22}}
\@writefile{toc}{\contentsline {subsubsection}{Codebook-learning loss}{22}{section*.21}\protected@file@percent }
\abx@aux@cite{0}{Barlow}
\abx@aux@segm{0}{0}{Barlow}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Modifying the VQVAE for Enhanced Self Supervised Learning}{23}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Barlow Twins}{23}{subsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Objective function of Barlow Twins}{23}{section*.22}\protected@file@percent }
\abx@aux@cite{0}{Barlow}
\abx@aux@segm{0}{0}{Barlow}
\abx@aux@cite{0}{Barlow}
\abx@aux@segm{0}{0}{Barlow}
\abx@aux@cite{0}{Barlow}
\abx@aux@segm{0}{0}{Barlow}
\@writefile{toc}{\contentsline {subsubsection}{Siamese architecture}{24}{section*.23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Illustration of the Barlow Twins procedure, inspired by the original paper\blx@tocontentsinit {0}\cite {Barlow}.\relax }}{24}{figure.caption.24}\protected@file@percent }
\newlabel{fig:Barlow}{{4.2}{24}{Illustration of the Barlow Twins procedure, inspired by the original paper\cite {Barlow}.\relax }{figure.caption.24}{}}
\newlabel{fig:Barlow@cref}{{[figure][2][4]4.2}{[1][24][]24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Modifying the VQVAE encoder}{24}{subsection.4.4.2}\protected@file@percent }
\abx@aux@cite{0}{VICReg}
\abx@aux@segm{0}{0}{VICReg}
\abx@aux@cite{0}{Barlow}
\abx@aux@segm{0}{0}{Barlow}
\abx@aux@cite{0}{SSLs}
\abx@aux@segm{0}{0}{SSLs}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Illustration of the Barlow Twins modification to VQVAE including the loss function calculation.\relax }}{25}{figure.caption.25}\protected@file@percent }
\newlabel{fig:BTVQVAE}{{4.3}{25}{Illustration of the Barlow Twins modification to VQVAE including the loss function calculation.\relax }{figure.caption.25}{}}
\newlabel{fig:BTVQVAE@cref}{{[figure][3][4]4.3}{[1][25][]25}}
\@writefile{toc}{\contentsline {subsubsection}{Learning}{25}{section*.26}\protected@file@percent }
\abx@aux@cite{0}{augs}
\abx@aux@segm{0}{0}{augs}
\abx@aux@cite{0}{SSLs}
\abx@aux@segm{0}{0}{SSLs}
\newlabel{eq:BTVQVAEloss}{{4.18}{26}{Learning}{equation.4.18}{}}
\newlabel{eq:BTVQVAEloss@cref}{{[equation][18][4]4.18}{[1][26][]26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Augmentation Techniques}{26}{subsection.4.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Visualization of the flip, slope and STFT augmentation aplied on a timeseries. \relax }}{27}{figure.caption.29}\protected@file@percent }
\abx@aux@cite{0}{UCRArchive2018}
\abx@aux@segm{0}{0}{UCRArchive2018}
\abx@aux@cite{0}{SSLs}
\abx@aux@segm{0}{0}{SSLs}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Experimental Setup}{28}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}UCR Archive}{28}{section.5.1}\protected@file@percent }
\abx@aux@cite{0}{SSLs}
\abx@aux@segm{0}{0}{SSLs}
\abx@aux@cite{0}{SSLs}
\abx@aux@segm{0}{0}{SSLs}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Chosen UCR Archive subset. Data collected by Lee et al\blx@tocontentsinit {0}\cite {SSLs}\relax }}{29}{table.caption.30}\protected@file@percent }
\newlabel{tab:sample_table}{{5.1}{29}{Chosen UCR Archive subset. Data collected by Lee et al\cite {SSLs}\relax }{table.caption.30}{}}
\newlabel{tab:sample_table@cref}{{[table][1][5]5.1}{[1][29][]29}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Reconstruction evaluation}{29}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Downstream evaluation}{29}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Extracting Discrete latent variables}{29}{subsection.5.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Illustration showing the processing of training and validation datasets to latent representations.\relax }}{30}{figure.caption.31}\protected@file@percent }
\newlabel{fig:latents}{{5.1}{30}{Illustration showing the processing of training and validation datasets to latent representations.\relax }{figure.caption.31}{}}
\newlabel{fig:latents@cref}{{[figure][1][5]5.1}{[1][30][]30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Downstream tests}{30}{subsection.5.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Results and discussion}{31}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusion}{32}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\abx@aux@read@bbl@mdfivesum{FED99EB9100443583DCCA050B9F73967}
\abx@aux@defaultrefcontext{0}{cvae}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{VICReg}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{SimCLR}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{svm}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{UCRArchive2018}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{BYOL}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{ResLearn}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{MoCo}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{batchnorm}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{1312.6114}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{VAE}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{SSLs}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{lee2023masked}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Siamese}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{CNNs}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{neuvqvae}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{RiebesellTikZ2022}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{VQVAE-2}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{ReLuGrad}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{posteriorcollapse}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{augs}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Barlow}{anyt/global//global/global}
\gdef \@abspage@last{41}
